{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gTUrVbQoeSw0",
        "wdtaEDEXkmaL",
        "FJ5uJbxDlN_i",
        "fpfRaejql39w"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning-based Phishing Detection"
      ],
      "metadata": {
        "id": "gTUrVbQoeSw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# read the csv file and create a pandas data frame\n",
        "df_raw = pd.read_csv(\"/content/High_Risk_URL_Dataset.csv\")"
      ],
      "metadata": {
        "id": "Fz203blAemQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw.head(10)"
      ],
      "metadata": {
        "id": "jFTTJP3sfXEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read the structured and labeled data\n",
        "df = pd.read_csv(\"/content/structured-labeled-phishing-URL-data.csv\")"
      ],
      "metadata": {
        "id": "KJJb9ZUUfa-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "4JL66EEehkP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "DtCz2CVmhl3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "51S9PR2Lh64W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.label.value_counts()"
      ],
      "metadata": {
        "id": "z_wzaIXrkYkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Between raw data & structured-labeled data\n",
        "- Data cleaning\n",
        "-- Duplicates, missing values\n",
        "- Feature extraction\n",
        "-- Run feature functions and obtain numerical value for each feature\n",
        "-- e.g: number of 'digits', 'words', '?' OR has 'query', 'subdomain' OR length of 'path', 'subdomain', 'host' etc.\n",
        "- Labeling\n",
        "-- Binary: 1 (Phishing) | 0 (Legitimate)"
      ],
      "metadata": {
        "id": "47ZgcJ8aiEJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare train and test data"
      ],
      "metadata": {
        "id": "wdtaEDEXkmaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Shuffle the DataFrame\n",
        "df = shuffle(df)\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = df.drop('label', axis=1)\n",
        "y = df['label']\n",
        "\n",
        "# Split the data into training and testing sets, maintaining class proportions\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Verify class distribution in the splits (optional)\n",
        "print(\"\\nClass Distribution in Training Set:\")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "print(\"\\nClass Distribution in Testing Set:\")\n",
        "print(y_test.value_counts())\n"
      ],
      "metadata": {
        "id": "SkLbfVuzkAW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "by_vm62VkoxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train two ML models"
      ],
      "metadata": {
        "id": "FJ5uJbxDlN_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression classifier\n",
        "lr_classifier = LogisticRegression(random_state=42, max_iter=1000) # Increased max_iter\n",
        "\n",
        "# Train the Random Forest classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Train the Logistic Regression classifier\n",
        "lr_classifier.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "fbFpR_iek1ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Make predictions on the test set for Random Forest\n",
        "rf_predictions = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate Random Forest performance\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "rf_precision = precision_score(y_test, rf_predictions)\n",
        "rf_recall = recall_score(y_test, rf_predictions)\n",
        "rf_f1 = f1_score(y_test, rf_predictions)\n",
        "\n",
        "print(\"Random Forest Performance:\")\n",
        "print(f\"Accuracy: {rf_accuracy:.4f}\")\n",
        "print(f\"Precision: {rf_precision:.4f}\")\n",
        "print(f\"Recall: {rf_recall:.4f}\")\n",
        "print(f\"F1-score: {rf_f1:.4f}\")\n",
        "\n",
        "\n",
        "# Make predictions on the test set for Logistic Regression\n",
        "lr_predictions = lr_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate Logistic Regression performance\n",
        "lr_accuracy = accuracy_score(y_test, lr_predictions)\n",
        "lr_precision = precision_score(y_test, lr_predictions)\n",
        "lr_recall = recall_score(y_test, lr_predictions)\n",
        "lr_f1 = f1_score(y_test, lr_predictions)\n",
        "\n",
        "print(\"\\nLogistic Regression Performance:\")\n",
        "print(f\"Accuracy: {lr_accuracy:.4f}\")\n",
        "print(f\"Precision: {lr_precision:.4f}\")\n",
        "print(f\"Recall: {lr_recall:.4f}\")\n",
        "print(f\"F1-score: {lr_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "FdSfmWGZlU5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: classification report\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate and print the classification report for Random Forest\n",
        "print(\"\\nRandom Forest Classification Report:\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "\n",
        "# Generate and print the classification report for Logistic Regression\n",
        "print(\"\\nLogistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, lr_predictions))\n"
      ],
      "metadata": {
        "id": "jn1ieLwIltdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the model"
      ],
      "metadata": {
        "id": "fpfRaejql39w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save the rf_classifier\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Save the trained Random Forest classifier to a file\n",
        "joblib.dump(rf_classifier, 'rf_classifier_model.pkl')\n"
      ],
      "metadata": {
        "id": "t81R3sQwl5D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved Random Forest model\n",
        "loaded_rf_model = joblib.load('rf_classifier_model.pkl')"
      ],
      "metadata": {
        "id": "jsmdinCKl_zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_index = 54  # Choose an index from X_train\n",
        "sample = X_train.iloc[[sample_index]]\n",
        "sample"
      ],
      "metadata": {
        "id": "xtLLGBNvmf6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_label = y_train.iloc[54]\n",
        "sample_label"
      ],
      "metadata": {
        "id": "BLZxzdWGmpJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction using the loaded model\n",
        "prediction = loaded_rf_model.predict(sample)\n",
        "print(f\"Prediction for the sample: {prediction[0]}\")"
      ],
      "metadata": {
        "id": "XXYmZ1lTmhkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the **problem** in this process?"
      ],
      "metadata": {
        "id": "AoQhCR1Em5S-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feature extraction time for some features is costly in real-time applications. How can we minimize the total cost of the feature extraction process within the context of an ML pipeline?"
      ],
      "metadata": {
        "id": "PpjqUcLYnGbH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zoc7UPOTmmUF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}